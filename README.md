# STIP

This repository is meant to host common functionalities of the STIP dataset. This includes, but is not limited to, preprocessing, annotation reading, visualisation, and any other miscellaneous functions.


## Processing Data

To process the data - requires the following steps.

1) Dump image frames of 12fps videos and 20 fps videos. The script visualise_tracks.py can be run with the -d flag to create these image dumps. THIS IS THE SLOWEST STEP OF THE PIPELINE

2) Create the mapping file between 12fps images and 20fps images. This can be done by running match_frames.py

3) Transfer the original 2fps annotations to 20fps, using the mapping file. This can be done using the script transfer_annotations.py

4) To run tracking, restructure the images + detections using convert_to_MOT.py - which changes the detection and GT format to match MOT, and then create_sequence_structure.py which changes the file organisation to serve as input to the tracker.

5) Run postprocess_tracks.py to correct systematic errors in tracker output. 

5) Run collate_tracks.py if you would like to merge outputs of all sequences into a single json file.


## Manual Correction

Manual correction is done using VoTT. This is an electron based app, and so you will need to install node.js;
Install instructions, as well as the repository to be used can be found here: 
https://github.com/piensa/VoTT/tree/logger

Once this is installed, you will require the original 20fps video in mp4 format (ONLY mp4 works, with H264 encoding, not MotionJPEG) - I've been reencoding the the mkv files we have using ffmpeg, and you can find the new videos in /cvgl2/u/ashenoi/STIP/20fps_mp4_videos.

To use VoTT you need the video and the json file to have the exact same name. For example is video file is example123.mp4, the JSON file needs to be example123.mp4.json.

The JSON file format needed is automatically generated by preprocess_tracks.py

I've found that the fastest way to perform manual correction requires two passes through each video. The first pass is to draw and correct boxes. This is to ensure that every target has a box, and that there are no extra boxes. Also, any correction to the box, such as size and location, should be done in this pass. 

The second pass is to make sure there is ID consistency between frames. More often than not, this requires very minimal corrections, but is still required. To do this, you will have to go through the sequence within VoTT, look at the field 'name', and look at the correspoding matchIds field to see the ID assigned. Changes need to be made directly in the json annotation file.

Make any corrections only after closing VoTT. If you make changes when VoTT is still open, your changes can be overwritten.

General instructions for annotation:

The box must contain the entire person. 
The box must not be unnecessarily large. Approximately half a head length above the head of the person, and a quarter of a head length below the feet of the person is recommended.
In case of occlusion, the best approximation of where the person is should be taken into account.
The size of the box should vary as smoothly as possible - this can be done in postprocessing once IDs have been reliably assigned.
Only draw boxes/keep boxes which are present in the original annotation - we do not care about any other persons in the scene.

